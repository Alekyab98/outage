from datetime import datetime, timedelta
from functools import partial
import os
import sys
import yaml
from airflow import DAG
from google.cloud import bigquery
from airflow.exceptions import AirflowException
from airflow.operators.python import PythonOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from airflow.operators.dummy import DummyOperator
from airflow.models import TaskInstance
from airflow.providers.google.cloud.sensors.gcs import GCSObjectsWithPrefixExistenceSensor
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook

BASE_DIR = "/home/airflow/gcs/dags/vz-it-gudv-dtwndo-0"
sys.path.append(f"{BASE_DIR}/qes_truecall/python")

from DO_utils import publishLog, create_do_dict

project = os.environ['GCP_PROJECT']
with open(f"{BASE_DIR}/qes_truecall/config/base_config.yaml", 'r') as file:
    base_config = yaml.full_load(file)

with open(f"{BASE_DIR}/qes_truecall/config/qos_performance_customer.yml", 'r') as file:
    dag_config = yaml.full_load(file)

config_values = {}

filtered_base_dict = dict(filter(lambda elem: elem[0] == project, base_config.items()))
filtered_dict = dict(filter(lambda elem: elem[0] == project, dag_config.items()))

if len(filtered_base_dict) > 0:
    base_value = filtered_base_dict[project][0]
    config_values = {**config_values, **base_value}
else:
    print("No config found exiting..")
    sys.exit(-1)
if len(filtered_dict) > 0:
    app_value = filtered_dict[project][0]
    config_values = {**config_values, **app_value}
else:
    print("No config found exiting..")
    sys.exit(-1)

source_name = config_values['source_name']
target_name = config_values['target_name']
GCP_PROJECT_ID = config_values['gcp_project']
bq_connection_id = config_values['google_cloud_conn_id']
region = config_values['region']
DAG_ID = config_values['dag_id']
base_directory = config_values['base_directory']
env = config_values['env']
dataset_id = config_values['dataset_id']
stored_proc = config_values['stored_proc']
table_name = config_values['table_name']
schedule_interval = config_values['schedule_interval']
priority = config_values['priority']

failure_email_alert_distro = config_values['failure_email_alert_distro']
gcs_bucket_location = config_values['gcs_bucket_location']
delimiter = config_values['delimiter']
truecall_missing_alert_dl = config_values['truecall_missing_alert_dl']
config_values['arrival_dt_hr'] ='arrival_dt_hr'
bucket_path = config_values['bucket_path']

do_dict = create_do_dict(config_values)

default_args = {
    'owner': 'dtwin',
    'depends_on_past': False,
    'start_date': datetime(year=2025, month=9, day=10, hour=00, minute=00),
    'email': [failure_email_alert_distro],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 0,
    'retry_delay': timedelta(minutes=3)
}

dag = DAG(
    dag_id=DAG_ID,
    schedule_interval=schedule_interval,
    catchup=True,
    default_args=default_args,
    description='This DAG call Stored Procedure',
    concurrency=int(config_values['concurrency']),
    max_active_runs=int(config_values['max_active_runs']),
    tags=["dtwin","qes","truecall"]
)

start = DummyOperator(task_id='start',
                      dag=dag,
                      on_success_callback=partial(publishLog, "PROGRESS", do_dict),
                      on_failure_callback=partial(publishLog, "FAILURE", do_dict))
                      
# --- Update the loop to create sensor tasks for the current hour intervals (00, 15, 30, 45) ---
sensors = []
for i in range(0, 60, 15):  # Loop through 00, 15, 30, 45 minutes
    task_id = f'check_success_file_{i:02d}min'
    prefix = bucket_path + f"/arrival={{{{ data_interval_end.strftime('%Y%m%d%H') }}}}{i:02d}/_SUCCESS"
    
    sensor = GCSObjectsWithPrefixExistenceSensor(
        dag=dag,
        task_id=task_id,
        bucket=gcs_bucket_location,
        prefix=prefix,
        mode='reschedule',
        poke_interval=900,
        timeout=7200,
        soft_fail=True,
        google_cloud_conn_id=bq_connection_id,
        on_failure_callback=partial(publishLog, "FAILURE", do_dict)
    )
    sensors.append(sensor)

# --- Update the check_all_file_statuses function ---
def check_all_file_statuses(**context):
    """
    Checks the status of all 4 upstream GCS sensors for the current hour
    and raises a single, consolidated error if any of them have failed.
    """
    process_date = context['data_interval_end'].strftime("%Y-%m-%d %H:%M:%S")
    ti = context['ti']
    ti.xcom_push(key="process_date", value=process_date)
    print("process_date:{}".format(process_date))

    # trans_date
    trans_date = context['data_interval_end'].strftime("%Y-%m-%d")
    ti.xcom_push(key="trans_date", value=trans_date)
    print("trans_date:{} ".format(trans_date))
    print("process_date::{},trans_date:{}".format(process_date, trans_date))    

    failed_checks = []

    # Loop through the current hour's intervals (00, 15, 30, 45 minutes)
    for i in range(0, 60, 15):
        sensor_task_id = f'check_success_file_{i:02d}min'
        arrival_time_str = context['data_interval_end'].strftime('%Y%m%d%H') + f'{i:02d}'  # Up to minutes only

        task_status = (TaskInstance(context['dag'].get_task(sensor_task_id), context['logical_date'])).current_state()
        if task_status != 'success':
            failed_checks.append(f"-> No Success Marker for minute {i:02d}. Path: gs://{gcs_bucket_location}/{bucket_path}/arrival={arrival_time_str}/_SUCCESS")

    if failed_checks:
        error_details = "\n\n".join(failed_checks)
        raise AirflowException(f"""Airflow exception raised here -> Missing data to run Truecall process\n
            {error_details}\n\n
            Please verify data load in Truecall 15 mins Source Table.\n
            Environment  : {env.upper()}\n
            Source_Table : {source_name}\n
            Process_Name : QES Truecall Hourly \n
            trans_date   : {trans_date}\n
            process_dt : {process_date}""")
    else:
        print("All 4 success markers found. Proceeding to stored procedure call.")
   
def run_truecall_sp(**kwargs):
    bq_hook = BigQueryHook(gcp_conn_id=bq_connection_id)
    client = bigquery.Client(project=GCP_PROJECT_ID, credentials=bq_hook.get_credentials())
    process_date = kwargs['data_interval_end'].strftime("%Y-%m-%d %H:%M:%S")
    trans_date = kwargs['data_interval_end'].strftime("%Y-%m-%d")
    arrival_dt_hr = kwargs['data_interval_end'].strftime("%Y%m%d%H")
    print("""trans_date:{},process_date::{},arrival_dt_hr:{}""".format(trans_date, process_date, arrival_dt_hr))
    print("Running store proc")
    sp = f"""CALL {dataset_id}.{stored_proc}('{trans_date}','{process_date}','{arrival_dt_hr}')"""
    print("Running stored proc:{}".format(sp))
    job_config = bigquery.QueryJobConfig(priority=priority)
    query_job_org = client.query(sp, job_config=job_config)
    query_job_org.result()
    
consolidated_status_check = PythonOperator(
    dag=dag,
    task_id="consolidated_status_check",
    python_callable=check_all_file_statuses,
    provide_context=True,
    trigger_rule="all_done",
    on_failure_callback=partial(publishLog, "FAILURE", do_dict)
)

call_truecall_sp = PythonOperator(
    dag=dag,
    task_id="call_truecall_sp",
    python_callable=run_truecall_sp,
    provide_context=True,
    on_failure_callback=partial(publishLog, "FAILURE", do_dict)
)

end = DummyOperator(task_id='end',
                    dag=dag,
                    trigger_rule='all_done',
                    on_success_callback=partial(publishLog, "SUCCESS", do_dict),
                    on_failure_callback=partial(publishLog, "FAILURE", do_dict))

# --- Task Dependencies ---
start >> sensors
consolidated_status_check.set_upstream(sensors)
consolidated_status_check >> call_truecall_sp >> end
