from datetime import datetime
from airflow import DAG
import os
import sys
import yaml
from airflow.providers.google.cloud.operators.bigquery import (
    BigQueryInsertJobOperator,
)
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from functools import partial
from datetime import datetime, timedelta
from functools import partial
from datetime import datetime, timedelta
from airflow.utils.trigger_rule import TriggerRule
from airflow.sensors.sql import SqlSensor
import time
from datetime import datetime, timedelta
from functools import partial
from airflow import DAG, AirflowException
import os
import sys
import yaml
from airflow.operators.dummy import DummyOperator
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from datetime import datetime, timedelta, timezone
import time
from airflow import DAG
import os
import sys
import yaml
from airflow.providers.google.cloud.operators.bigquery import (
    BigQueryInsertJobOperator,BigQueryCheckOperator,
)
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.dummy import DummyOperator
from functools import partial
from airflow.providers.google.cloud.operators.dataproc import DataprocCreateBatchOperator, DataprocSubmitJobOperator
from airflow.utils.trigger_rule import TriggerRule
from airflow.exceptions import AirflowFailException


BASE_DIR = "/home/airflow/gcs/dags/vz-it-gudv-dtwndo-0"
sys.path.append(f"{BASE_DIR}/outage_line/python")


#from vz_de_common_observability_integration import publishLog, create_do_dict
from DO_utils import publishLog, create_do_dict

project = os.environ['GCP_PROJECT']
with open(f"{BASE_DIR}/outage_line/config/base_config.yml", 'r') as file:
    base_config = yaml.full_load(file)

with open(f"{BASE_DIR}/outage_line/config/gudv_outage_line.yml", 'r') as file:
    dag_config = yaml.full_load(file)

config_values = {}


filtered_base_dict = dict(filter(lambda elem: elem[0] == project, base_config.items()))
filtered_dict = dict(filter(lambda elem: elem[0] == project, dag_config.items()))

if len(filtered_base_dict) > 0:
    base_value = filtered_base_dict[project][0]
    config_values = {**config_values, **base_value}
else:
    print("No config found exiting...")
    sys.exit(-1)
if len(filtered_dict) > 0:
    app_value = filtered_dict[project][0]
    config_values = {**config_values, **app_value}
else:
    print("No config found exiting...")
    sys.exit(-1)

GCP_PROJECT_ID = config_values['gcp_project']
bq_connection_id = config_values['google_cloud_conn_id']
region = config_values['region']
DAG_ID = config_values['dag_id']
base_directory = config_values['base_directory']
env = config_values['env']
dataset_id = config_values['dataset_id']
stored_proc = config_values['stored_proc']
table_name = config_values['table_name']
raw_src_tbl= config_values['raw_src_tbl']
raw_src_list= raw_src_tbl.split('.')
raw_src_project_id= raw_src_list[0]
raw_src_dataset_id= raw_src_list[1]
raw_src_tbl_name= raw_src_list[2]
mdns_src_tbl= config_values['mdns_src_tbl']
mdns_src_list= mdns_src_tbl.split('.')
mdns_src_project_id= mdns_src_list[0]
mdns_src_dataset_id= mdns_src_list[1]
mdns_src_tbl_name= mdns_src_list[2]
schedule_interval = config_values['schedule_interval']
failure_email_alert_distro = config_values['failure_email_alert_distro']



trans_date = '{{ (macros.datetime.strptime(data_interval_end.strftime("%Y-%m-%d"),"%Y-%m-%d") - macros.dateutil.relativedelta.relativedelta(days=1)).strftime("%Y-%m-%d") }}'
process_date= '{{ (macros.datetime.strptime(data_interval_end.strftime("%Y-%m-%d %H"),"%Y-%m-%d %H")).strftime("%Y-%m-%d")}}'

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(year=2025, month=4, day=30, hour=9, minute=00),
    'email': [failure_email_alert_distro],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=3)
}

dag = DAG(
    dag_id=DAG_ID,
    schedule_interval=schedule_interval,
    catchup=True,
    default_args=default_args,
    description='This DAG call Stored Procedure',
    concurrency=int(config_values['concurrency']),
    max_active_runs=int(config_values['max_active_runs']),
    tags=["dtwin","nw_research_assistant","network_genie","outage_line","auto_bi"]
)


do_dict = create_do_dict(config_values)

start = DummyOperator(task_id='start',
                      dag=dag,
                      on_success_callback=partial(publishLog, "PROGRESS", do_dict),
                      on_failure_callback=partial(publishLog, "FAILURE", do_dict))

check_nqes_outage_line_src_tbl_count_task = BigQueryCheckOperator(
        task_id="check_nqes_outage_line_src_tbl_count_task",
        dag=dag,
        gcp_conn_id=bq_connection_id,
        sql=f""" SELECT ( CASE WHEN (select if(count(*)>0 ,1,0) from {raw_src_project_id}.{raw_src_dataset_id}.{raw_src_tbl_name} where cast(trans_dt as date) =cast('{trans_date}' as date)) > 0  THEN 1 ELSE 0 END +  CASE WHEN (select if(count(*)>0 ,1,0) from {mdns_src_project_id}.{mdns_src_dataset_id}.{mdns_src_tbl_name} where cast(trans_dt as date) =cast('{trans_date}' as date)) > 0  THEN 1 ELSE 0 END )=2 """,
        use_legacy_sql= False,
        retries=6,
        retry_delay= timedelta(minutes=30),
        on_failure_callback=partial(publishLog, "FAILURE", do_dict))

call_outage_line_sp = BigQueryInsertJobOperator(
        task_id="call_outage_line_sp",
        dag=dag,
        gcp_conn_id=bq_connection_id,
        configuration={
                         "query": {
                              "query": f"CALL {dataset_id}.{stored_proc}('{trans_date}','{process_date}')",
                              "useLegacySql": False,
                              }
                         },
        on_failure_callback=partial(publishLog, "FAILURE", do_dict)

)
end = DummyOperator(task_id='end',
                    dag=dag,
                    on_success_callback=partial(publishLog, "SUCCESS", do_dict),
                    on_failure_callback=partial(publishLog, "FAILURE", do_dict))

start  >> check_nqes_outage_line_src_tbl_count_task >>  call_outage_line_sp >> end
